<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>James Blog</title>
  
  
  <link href="http://example.com/atom.xml" rel="self"/>
  
  <link href="http://example.com/"/>
  <updated>2023-09-17T23:48:41.997Z</updated>
  <id>http://example.com/</id>
  
  <author>
    <name>Haowen(James) Ji</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Practical Machine Learning (Stanford-cs329)</title>
    <link href="http://example.com/2023/08/26/Learning/"/>
    <id>http://example.com/2023/08/26/Learning/</id>
    <published>2023-08-26T18:20:54.000Z</published>
    <updated>2023-09-17T23:48:41.997Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Section-I-What-a-data-scientist-do"><a href="#Section-I-What-a-data-scientist-do" class="headerlink" title="Section I: What a data scientist do?"></a>Section I: What a data scientist do?</h2><p> <img src="/2023/08/26/Learning/ds.png" alt="Fig.1"></p><h2 id="1-3-Data-Labeling"><a href="#1-3-Data-Labeling" class="headerlink" title="1.3 Data Labeling"></a>1.3 Data Labeling</h2><p> <img src="/2023/08/26/Learning/dl.png" alt="Fig.1"></p><h3 id="Semi-Supervised-Learning-SSL"><a href="#Semi-Supervised-Learning-SSL" class="headerlink" title="Semi-Supervised Learning (SSL)"></a>Semi-Supervised Learning (SSL)</h3><ul><li>Scenario: a <strong>small</strong> amount of labeled data, along with <strong>large</strong> amount of unlabeled data</li><li>Make assumptions on <font color="#0000dd"><strong>data distribution</strong></font><br>  to use unlabeled data<ul><li><strong>Continuity assumption</strong></li><li><strong>Cluster assumption</strong> </li><li><strong>Manifold assumption</strong>: data lie on a manifold of much lower dimension than the input space</li></ul></li></ul><h3 id="Self-training"><a href="#Self-training" class="headerlink" title="Self-training"></a>Self-training</h3><ul><li>A type of SSL<ol><li>Use labeled data train a model</li><li>Utilise the model to predict unlabeled data and get <strong>pseudo-labeled data</strong> (Only keep highly confident predictions)</li><li>Merge the newly labled data to train a better model</li></ol></li><li>Expensive model can be applied<ul><li>Deep NN, model ensemble/bagging</li></ul></li></ul><h3 id="Label-through-corwdsourcing"><a href="#Label-through-corwdsourcing" class="headerlink" title="Label through corwdsourcing"></a>Label through corwdsourcing</h3><ul><li>Hiring workers to label</li><li>e.g.: ImageNet, Amazon Mechanical Turk</li></ul><h3 id="Challenges"><a href="#Challenges" class="headerlink" title="Challenges:"></a>Challenges:</h3><ul><li>Simpler user interaction for wokers</li><li>Reduce tasks sent to labelers to save cost<ul><li><strong>Active learning</strong>: Model select the most “interesting” data for labelers</li><li>SSL with human intervention, only send them particular unlabeled data (most uncertain, prediction score is low)</li><li>Utilizing expensive model (query-by-commitee, train multipe models)</li></ul></li><li>Quality control</li></ul><h3 id="Weak-supervision"><a href="#Weak-supervision" class="headerlink" title="Weak supervision"></a>Weak supervision</h3><ul><li>Data programming: heuristic programs to assign labels<ul><li>Keyword search, pattern matching, third-party models</li><li>E.g. design rules to check if YouTube comments are spam or ham<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">check_out</span>(<span class="params">x</span>):</span><br><span class="line">  <span class="keyword">return</span> SPAM <span class="keyword">if</span> <span class="string">&quot;check out&quot;</span> <span class="keyword">in</span> x.lower() <span class="keyword">else</span> ABSTAIN</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">sentiment</span>():</span><br><span class="line">  <span class="keyword">return</span> HAM <span class="keyword">if</span> sentiment_polarity(x) &gt; <span class="number">0.9</span> <span class="keyword">else</span> ABSTAIN</span><br></pre></td></tr></table></figure></li></ul></li></ul><p><img src="/2023/08/26/Learning/dp.png" alt="Fig.2"></p><h2 id="2-1-Data-Analysis"><a href="#2-1-Data-Analysis" class="headerlink" title="2.1 Data Analysis"></a>2.1 Data Analysis</h2><h2 id="2-2-Data-Cleaning"><a href="#2-2-Data-Cleaning" class="headerlink" title="2.2 Data Cleaning"></a>2.2 Data Cleaning</h2><h3 id="Data-Errors"><a href="#Data-Errors" class="headerlink" title="Data Errors"></a>Data Errors</h3><ul><li>Data often have errors - the mismatch with ground truth (missing, erroneous or extreme values)</li><li>Good ML models are robust to errors<ul><li>DNN trained with SGD VS Decision trees </li></ul></li><li>Consequences:<ul><li>The training may still converge, but slower</li><li>Accuracy degradation, could be hard to detect</li><li>Deploying these models may impact the quality of the new collected data <ul><li>e.g. positive examples generated by poor recommendation / search results</li></ul></li></ul></li></ul><h3 id="Types-of-Data-Errors"><a href="#Types-of-Data-Errors" class="headerlink" title="Types of Data Errors"></a>Types of Data Errors</h3><ul><li>Outliers: data values that deviate from the distribution of values in the column </li><li>Rule violations: violate integrity constraints such as “Not Null” and “Must be unique”</li><li>Pattern violations: violate syntactic and semantic constraints such as formatting, misspell</li></ul><h3 id="Rule-based-Detection"><a href="#Rule-based-Detection" class="headerlink" title="Rule-based Detection"></a>Rule-based Detection</h3><ul><li><strong>Functional dependencies</strong>: <script type="math/tex">x \rightarrow y</script> means a value determines a unique value <ul><li>E.g. zip code $\rightarrow$ state, EIN $\rightarrow$ company name</li></ul></li><li>Denial constraints: specified with more flexible first-order logic <ul><li>Phone number is not empty if vendor has an EIN</li><li>If two captures of the same animal indicated by the same tag number, then the first one must be marked as original</li></ul></li></ul><h3 id="Pattern-based-Detection"><a href="#Pattern-based-Detection" class="headerlink" title="Pattern-based Detection"></a>Pattern-based Detection</h3><ul><li>Syntactic patterns <ul><li>Map a column to the most prominent data type and identify values do not fit</li><li>eng, en, english -&gt; English</li></ul></li><li>Semantic patterns<ul><li>Add rules through knowledge graph</li><li>Values in column “Country” need have capitals</li></ul></li></ul><h2 id="2-3-Data-Transformation"><a href="#2-3-Data-Transformation" class="headerlink" title="2.3 Data Transformation"></a>2.3 Data Transformation</h2><ul><li>Data are transformed into forms appropriate for ML algorithms</li><li>Data transformation methods for different data types:</li></ul><ol><li><strong>Min-max normalization</strong>: linearly map to a new min a and max b <ul><li>$x_i’ = \frac{x_i - min_x}{max_x - min_x} (b-a)+a$ </li><li>machine leanring may be sensitive to numbers, need to scale to a new range</li></ul></li><li><strong>Z-score normalization</strong>: 0 mean, 1 standard deviation<ul><li>$x_i’ = \frac{x_i - meanm(x)}{std(x)}$ </li><li>most common way</li></ul></li><li><strong>Decimal scaling</strong>:<ul><li>$x_i’ = x_i/10^j $ smallest $j$ s.t. $max(|x’|) &lt; 1$</li></ul></li><li><strong>Log scaling</strong>:<ul><li>$x_i’ = log(x_i)$</li><li>utilize when value varies a lot, turn +/- to */</li><li>e.g. the estimation for house price should be within 1%, 1,000$ is different for 10,000$ and 1000,000</li></ul></li></ol><h3 id="Image-Transformations"><a href="#Image-Transformations" class="headerlink" title="Image Transformations"></a>Image Transformations</h3><ul><li>Downsampling and cropping<ul><li>Save storage cost, faster loading at training</li><li>ML is good at low-resolution images</li><li>Be aware of image quality: Medium (80%-90%) jpeg compression may lead to 1% acc drop in ImageNet, better keep it over 90%</li></ul></li><li>Image whitening<ul><li>Whitening removes redundancy through linear transformations</li><li>Model converge faster, especially for unsupervised learning</li></ul></li></ul><h3 id="Video-transformations"><a href="#Video-transformations" class="headerlink" title="Video transformations"></a>Video transformations</h3><ul><li>Challenge:<ul><li>Input varies a lot (decode and size)</li></ul></li><li>Utilizing tractable ML problems with short video clips (&lt;10sec) </li><li>Common practice: decode a playable video clip, sample a sequence of frames<ul><li>Best for loading but need more space</li><li>Can use GPU to decode and apply image transformation</li></ul></li></ul><h3 id="Text-transformations"><a href="#Text-transformations" class="headerlink" title="Text transformations"></a>Text transformations</h3><ul><li><p>Stemming and lemmatization: a word $\rightarrow$ a common base form</p><ul><li>E.g. am, are, is $\rightarrow$ be     car, cars, car’s, cars’ $\rightarrow$ car</li></ul></li><li><p>Tokenization: text string $\rightarrow$ a list of tokens (smallest unit to ML algorithms)</p><ul><li>By word: text.split(‘ ‘)</li><li>By char: text.split(‘’)</li><li>By subwords: <ul><li>e.g. “a new gpu!” “a”, “new”, “gp”, “##u”, “!”</li><li>Custom vocabulary learned from the text corpus (Unigram, WordPiece)</li></ul></li></ul></li></ul><h2 id="2-4-Feature-Engineering"><a href="#2-4-Feature-Engineering" class="headerlink" title="2.4 Feature Engineering"></a>2.4 Feature Engineering</h2><ul><li>Feature engineering (FE) is the key to ML models before deep learning (DL), as ML prefer well defined fixed length input <ul><li>Traditional CV: detect corners / interest points.. </li></ul></li><li>DL train deep neural networks to automatically extract features <ul><li>Train CNN to replace feature extractor </li><li>Features are more relevant to the final task</li><li>Limitation: data hungry, computation heavy</li></ul></li></ul><h3 id="Tabular-Data-Features"><a href="#Tabular-Data-Features" class="headerlink" title="Tabular Data Features"></a>Tabular Data Features</h3><p><strong>Tabular data</strong> are in the form of a table, feature columns of numeric / categorical / string type </p><ul><li><strong>Int/float</strong>: directly use or or bin to unique int values</li><li><strong>Categorical data</strong>: one-hot encoding <ul><li>Map rare categories into “Unknown”</li></ul></li><li><strong>Date-time</strong>: a feature list such as <ul><li>[year, month, day, day_of_year, week_of_year, day_of_week]</li></ul></li><li><strong>Feature combination</strong>: Cartesian product of two feature groups <ul><li>[cat, dog] x [male, female] -&gt; [(cat, male), (cat, female), (dog, male), (dog, female)]</li></ul></li></ul><h3 id="Text-Features"><a href="#Text-Features" class="headerlink" title="Text Features"></a>Text Features</h3><ul><li>Represent text as <strong>token features</strong><ul><li>Bag of words (BoW) model (one-hot code encode every word)<ul><li>Limitations: needs careful <em>vocabulary design</em>, <em>missing context</em> </li></ul></li><li>Word Embeddings (e.g. Word2vec): <ul><li>Vectorizing words such that similar words are placed close together</li><li>Trained by predicting target word from context words</li></ul></li></ul></li><li>Pre-trained universal language models (e.g. universal sentence encoder, BERT, GPT-3)<ul><li><em>Giant</em> transformer models</li><li>Trained with large amount of unannotated data</li><li>Fine-tuning for downstream tasks</li></ul></li></ul><h3 id="Image-Video-Features"><a href="#Image-Video-Features" class="headerlink" title="Image/Video Features"></a>Image/Video Features</h3><ul><li>Traditionally extract images by hand-craft features such as SIFT</li><li>Now pre-trained deep neural networks are common used as feature extractor<ul><li>ResNet: trained with ImageNet (image classification)</li><li>I3D: trained with Kinetics (action classification) </li></ul></li></ul><h2 id="3-1-Machine-Learning-Overview"><a href="#3-1-Machine-Learning-Overview" class="headerlink" title="3.1 Machine Learning Overview"></a>3.1 Machine Learning Overview</h2><ul><li>Supervised: Train on labeled data to predict labels</li><li>Semi-supervised: Train on both labeled and unlabeled data, use models to infer labels for unlabeled data </li><li>Unsupervised: Train on unlabeled data</li><li>Reinforcement learning:  Use observations from the interaction with the <em>environment</em> to take actions to maximize reward</li></ul><h3 id="Supervised-Training"><a href="#Supervised-Training" class="headerlink" title="Supervised Training"></a>Supervised Training</h3><ul><li>Model: parameterized function to map inputs to label</li><li>Loss:  measurement of model prediction</li><li>Objective:  The goal to optimize model params for<ul><li>E.g. minimize the sum of losses over examples</li></ul></li><li><p>Optimization: The algorithm for solving the objective</p></li><li><p><strong>Types</strong>:</p><ul><li>Decision tree</li><li>Linear methods</li><li>Kernal machines</li><li>Neural Networks</li></ul></li></ul><h2 id="3-2-Decision-Trees"><a href="#3-2-Decision-Trees" class="headerlink" title="3.2 Decision Trees"></a>3.2 Decision Trees</h2><ul><li>Pros:<ul><li>Explainable</li><li>Suitable for numerical and categorical features</li></ul></li><li>Cons:<ul><li>Non-robust, senesitive to data (ensemble learning)</li><li>Overfiitting for complex trees (prune trees)</li><li>Not easy parallelized</li></ul></li></ul><h3 id="Random-Forest"><a href="#Random-Forest" class="headerlink" title="Random Forest"></a>Random Forest</h3><ul><li>Train multiple decision trees to improve robustness <ul><li>Trees are trained independently in parallel </li><li>Majority voting for classification, average for regression </li></ul></li><li>Where is the randomness from?<ul><li>Bagging: <em>randomly</em> sample training examples with replacement <ul><li>E.g. get [1,2,3,4,5] first, then replace it by [1,2,2,3,4]</li></ul></li><li>Randomly select a subset of features </li></ul></li></ul><h3 id="Gradient-Boosting-Decision-Trees"><a href="#Gradient-Boosting-Decision-Trees" class="headerlink" title="Gradient Boosting Decision Trees"></a>Gradient Boosting Decision Trees</h3><ul><li>Train multiple trees sequentially</li><li>At step t = 1, …, denote by $F_t(x)$ the sum of past trained trees<ul><li>Train a new tree $f_t$ on <strong>residuals</strong>: ${(x_i, y_i - F_t(x_i))}_{i=1}$ </li><li>$F_{t+1}(x) = F_t(x) + f_t(x)$</li></ul></li><li>The residual equals to $-\partial L/ \partial F$ if using MSE, so called gradient boosting</li></ul><h2 id="3-3-Linear-Methods"><a href="#3-3-Linear-Methods" class="headerlink" title="3.3 Linear Methods"></a>3.3 Linear Methods</h2><h3 id="Linear-regression"><a href="#Linear-regression" class="headerlink" title="Linear regression"></a>Linear regression</h3><ul><li>Given data $x = [x_1,x_2, …, x_p]$, linear regression predicts $\hat{y} = w_1x_1 + w_2x_2 + … + w_px_p + b = <w,x> + b$</w,x></li><li>w and b are learned from data</li><li><strong>Objective</strong>: minimize the mean square error (MSE, $\frac{1}{n}\sum(y_i - \hat{y_i})^2$)</li></ul><h3 id="Linear-classification"><a href="#Linear-classification" class="headerlink" title="Linear classification"></a>Linear classification</h3><ul><li>Outputs: vector outputs, i-th output reflects confidence score</li><li>Label: One-hot code, $y_i$ = 1 if it is the class, like [0,1,0]</li><li>Learn linear model for each class $o_i = <x, w_i> + b_i$</x,></li><li><strong>Objective</strong>: Minimize MSE loss $\frac{1}{m}||o - y||^2_2$</li></ul><h3 id="Softmax-Regression"><a href="#Softmax-Regression" class="headerlink" title="Softmax Regression"></a>Softmax Regression</h3><h2 id="5-1-Bias-amp-Variance"><a href="#5-1-Bias-amp-Variance" class="headerlink" title="5.1 Bias &amp; Variance"></a>5.1 Bias &amp; Variance</h2><ul><li>Sample data $D = {(x_1, y_1),…,(x_n, y_n)}$ from $y = f(x) + \epsilon$</li><li>Learn $\hat{f}$ from D by minimizing MSE, wanting it generates well over different D</li><li><script type="math/tex; mode=display">E_D[(y-\hat{f}(x))^2] = Bias[\hat{f}]^2 + Var[\hat{f}] + \sigma^2</script></li></ul><h3 id="Reduce-Bias-amp-Variance"><a href="#Reduce-Bias-amp-Variance" class="headerlink" title="Reduce Bias &amp; Variance"></a>Reduce Bias &amp; Variance</h3><div class="table-container"><table><thead><tr><th style="text-align:center">Reduce bias</th><th style="text-align:center">Reduce variance</th><th style="text-align:center">Reduce $\sigma^2$</th></tr></thead><tbody><tr><td style="text-align:center">More complex models</td><td style="text-align:center">Simpler &amp; Regularization</td><td style="text-align:center">Improve data</td></tr><tr><td style="text-align:center">Boosting</td><td style="text-align:center">Bagging</td><td style="text-align:center"></td></tr><tr><td style="text-align:center">Stacking</td><td style="text-align:center">Stacking</td></tr></tbody></table></div><ul><li>Reduce bias<ul><li>A more complex model</li><li>Boosting</li></ul></li></ul><h2 id="5-2-Bagging"><a href="#5-2-Bagging" class="headerlink" title="5.2 Bagging"></a>5.2 Bagging</h2><h3 id="Bagging-Bootstrap-AGGrgratING"><a href="#Bagging-Bootstrap-AGGrgratING" class="headerlink" title="Bagging - Bootstrap AGGrgratING"></a>Bagging - Bootstrap AGGrgratING</h3><ul><li>Bagging trains n base learners in parallel, combine to reduce model variance</li><li>Make decision by averaging the outputs (regression) or majority voting (classification)</li><li>Each base learner is trained on a bootstrap sample <ul><li>Assume m trainning examples, then randomly sampling m examples with replacement</li><li>Around $1-1/e \approx 63%$ unique examples will be sampled use the out-of-bag examples for validation</li></ul></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Bagging</span>: </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, base_learner, n_learners</span>):</span><br><span class="line">        self.learners = [clone(base_learner) <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(n_learners)]</span><br><span class="line"> </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">fit</span>(<span class="params">self, X, y</span>):</span><br><span class="line">        <span class="keyword">for</span> learner <span class="keyword">in</span> self.learners: </span><br><span class="line">            examples = np.random.choice(</span><br><span class="line">                np.arange(<span class="built_in">len</span>(X)), <span class="built_in">int</span>(<span class="built_in">len</span>(X)), replace=<span class="literal">True</span>) </span><br><span class="line">            learner.fit(X.iloc[examples, :], y.iloc[examples])</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">predict</span>(<span class="params">self, X</span>):</span><br><span class="line">        preds = [learner.predict(X) <span class="keyword">for</span> learner <span class="keyword">in</span> self.learners] </span><br><span class="line">        <span class="keyword">return</span> np.array(preds).mean(axis=<span class="number">0</span>)</span><br></pre></td></tr></table></figure><h3 id="Apply-bagging-with-unstable-Learners"><a href="#Apply-bagging-with-unstable-Learners" class="headerlink" title="Apply bagging with unstable Learners"></a>Apply bagging with unstable Learners</h3><ul><li>Bagging reduces model variance, especially for unstable learners </li><li>Given ground truth $f$ and base learner $h$ for regression, bagging prediction: $\hat{f}(x) = E[h(x)]$</li><li>Given $(E[x])^2 \leq E[x^2]$, we have<ul><li>$(f(x)-\hat{f}(x))^2 \leq E[(f(x)-h(x))^2]$</li></ul></li></ul><h2 id="5-3-Boosting"><a href="#5-3-Boosting" class="headerlink" title="5.3 Boosting"></a>5.3 Boosting</h2><ul><li>Learn weak learners sequentially, combine to reduce model bias</li><li>At step i, repeat:<ul><li>Train a weak learner h_i, evaluate its errors $\epsilon _t$<ul><li>AdaBoost: Re-sample data according to $\epsilon _t$ focus on wrongly predicted samples</li><li>Gradient boosting: Train learner to predict </li></ul></li></ul></li></ul><h3 id="Gradient-Boosting"><a href="#Gradient-Boosting" class="headerlink" title="Gradient Boosting"></a>Gradient Boosting</h3><ul><li>Denote by $H_t(x)$ the model at time $t$, with $H_1(x) = 0$</li><li>At step $t = 1, 2, …$<ul><li>Train a new model $h_t$ on residuals: ${(x_i, y_i - H_t(x_i))}_{i=1,…,m}$</li><li>$H_{t+1}(x) = H_t(x) + \eta h_t(x)$ ($\eta$ is learning rate )</li></ul></li><li>The residuals equal to $-\partial L / \partial H$ if using MSE</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">GradientBoosting</span>: </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, base_learner, n_learners, learning_rate</span>):</span><br><span class="line">        self.learners = [clone(base_learner) <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(n_learners)]</span><br><span class="line">        self.lr = learning_rate</span><br><span class="line"> </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">fit</span>(<span class="params">self, X, y</span>):</span><br><span class="line">        residual = y.copy()</span><br><span class="line">        <span class="keyword">for</span> learner <span class="keyword">in</span> self.learners:</span><br><span class="line">            learner.fit(X, residual) </span><br><span class="line">            residual -= self.lr * learner.predict(X) </span><br><span class="line"> </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">predict</span>(<span class="params">self,X</span>):</span><br><span class="line">        preds = [learner.predict(X) <span class="keyword">for</span> learner <span class="keyword">in</span> self.learners]</span><br><span class="line">        <span class="keyword">return</span> np.array(preds).<span class="built_in">sum</span>(axis=<span class="number">0</span>) * self.lr</span><br></pre></td></tr></table></figure><h3 id="Gradient-Boosting-Decision-Trees-GBDT"><a href="#Gradient-Boosting-Decision-Trees-GBDT" class="headerlink" title="Gradient Boosting Decision Trees (GBDT)"></a>Gradient Boosting Decision Trees (GBDT)</h3><ul><li>Use week learner in decision tree (small max_depth and randomly sampling features)</li><li>Sequentially constructing trees runs slow</li></ul><h2 id="5-4-Stacking"><a href="#5-4-Stacking" class="headerlink" title="5.4 Stacking"></a>5.4 Stacking</h2><ul><li>Combine multiple base learners to reduce variance<ul><li>Base learners can be different model types</li><li>Linearly combine base learners outputs by learned parameters</li></ul></li><li>bagging VS stacking<ul><li>Bagging: bootstrap samples to get diversity with same type models</li><li>Stacking: different types of models extract different features</li></ul></li></ul><h3 id="Multi-layer-Stacking"><a href="#Multi-layer-Stacking" class="headerlink" title="Multi-layer Stacking"></a>Multi-layer Stacking</h3><ul><li>Stacking base learners in multiple levels to reduce bias<ul><li>Can use a different set of base learners at each level</li></ul></li><li>Upper levels (e.g. L2) are trained on the outputs of the level below (e.g. L1)</li></ul><h3 id="Overfitting-in-Multi-layer-Stacking"><a href="#Overfitting-in-Multi-layer-Stacking" class="headerlink" title="Overfitting in Multi-layer Stacking"></a>Overfitting in Multi-layer Stacking</h3><ul><li>Train leaners from different levels on different data to alleviate overfitting <ul><li>Split training data into A and B, train L1 learners on A, predict on B to generate inputs for L2</li></ul></li><li>Repeated $k$-fold bagging:<ul><li>Train $k$ models as in $k$-fold cross validation</li><li>Combine predictions of each model on out-of-fold data</li><li>Repeat step 1,2 by times, average the predictions of each example for the next level training</li></ul></li></ul><h2 id="6-1-Model-Tuning"><a href="#6-1-Model-Tuning" class="headerlink" title="6.1 Model Tuning"></a>6.1 Model Tuning</h2><h3 id="Automated-Machine-Learning-AutoML"><a href="#Automated-Machine-Learning-AutoML" class="headerlink" title="Automated Machine Learning (AutoML)"></a>Automated Machine Learning (AutoML)</h3><ul><li>Automate every step in applying ML to solve real-world problems: data cleaning, feature extraction, model selection…</li><li>Hyperparameter optimization (HPO): find a good set of hyperparameters through search algorithms</li><li>Neural architecture search (NAS): construct a good neural network model </li></ul><h2 id="6-2-HPO-algorithms"><a href="#6-2-HPO-algorithms" class="headerlink" title="6.2 HPO algorithms"></a>6.2 HPO algorithms</h2><h3 id="Search-Space"><a href="#Search-Space" class="headerlink" title="Search Space"></a>Search Space</h3><h3 id="HPO-algorithms-Black-box-or-Multi-fidelity"><a href="#HPO-algorithms-Black-box-or-Multi-fidelity" class="headerlink" title="HPO algorithms: Black-box or Multi-fidelity"></a>HPO algorithms: Black-box or Multi-fidelity</h3><ul><li>Black-box: treats a training job as a black-box in HPO: <ul><li>Completes the training process for each trial</li></ul></li><li>Multi-fidelity: modifies the training job to speed up the search <ul><li>Train on subsampled datasets</li><li>Reduce model size (e.g less #layers, #channels)</li><li>Stop bad configuration earlier</li></ul></li></ul><h3 id="Two-common-HPO-strategies"><a href="#Two-common-HPO-strategies" class="headerlink" title="Two common HPO strategies"></a>Two common HPO strategies</h3><ul><li><p>Grid search</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> config <span class="keyword">in</span> search_space:</span><br><span class="line">    train_and_eval(config) </span><br><span class="line"><span class="keyword">return</span> best_result</span><br></pre></td></tr></table></figure><ul><li>All combinations are evaluated</li><li>Guarantees the best results</li><li>Curse of dimensionality</li></ul></li><li><p>Random search</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(n):</span><br><span class="line"> config = random_select(search_space)</span><br><span class="line"> train_and_eval(config) </span><br><span class="line"><span class="keyword">return</span> best_result</span><br></pre></td></tr></table></figure><ul><li>Random combinations are tried</li><li>More efficient than grid search </li></ul></li></ul><h3 id="Bayesian-Optimization-BO"><a href="#Bayesian-Optimization-BO" class="headerlink" title="Bayesian Optimization (BO)"></a>Bayesian Optimization (BO)</h3><ul><li>BO: Iteratively learn a mapping from HP to objective function. Based on previous trials. Select the next trial based on the current estimation. </li><li>Surrogate model <ul><li>Estimate how the objective function depends on HP </li><li>Probabilistic regression models: Random forest, Gaussian process, …</li></ul></li><li>Acquisition function<ul><li>Acquisition max means uncertainty and predicted objective are high. </li><li>Sample the next trial according to the acquisition function</li><li>Trade off exploration and exploitation</li></ul></li><li>Limitation of BO: <ul><li>In the initial stages, similar to random search</li><li>Optimization process is sequential</li></ul></li></ul><h3 id="Successive-Halving"><a href="#Successive-Halving" class="headerlink" title="Successive Halving"></a>Successive Halving</h3><ul><li>Save the budget for most promising config<ol><li>Randomly pick n configurations to train m epochs</li><li>Repeat until one configuration left:<ol><li>Keep the best n/2 configuration to train another m epochs</li><li>Keep the best n/4 configuration to train another 2m epochs</li><li>……</li></ol></li></ol></li><li>Select and based on training budget and #epoch needed for a full training</li></ul><h3 id="Hyperband"><a href="#Hyperband" class="headerlink" title="Hyperband"></a>Hyperband</h3><ul><li>In Successive Halving <ul><li>n: exploration </li><li>m: exploitation </li></ul></li><li>Hyperband runs multiple Successive Halving, each time decreases and increases </li><li>More exploration first, then do more exploit</li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;Section-I-What-a-data-scientist-do&quot;&gt;&lt;a href=&quot;#Section-I-What-a-data-scientist-do&quot; class=&quot;headerlink&quot; title=&quot;Section I: What a data s</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>Algorithms</title>
    <link href="http://example.com/2023/06/26/Algorithms/"/>
    <id>http://example.com/2023/06/26/Algorithms/</id>
    <published>2023-06-26T19:35:04.000Z</published>
    <updated>2023-08-29T13:18:55.739Z</updated>
    
    <content type="html"><![CDATA[<p>The great algorithms are the poetry of computation - Francis Sullivan</p><h1 id="Section-I-Algorithms-Analysis"><a href="#Section-I-Algorithms-Analysis" class="headerlink" title="Section I: Algorithms Analysis"></a>Section I: Algorithms Analysis</h1><h3 id="1-Framework-for-predicting-performance-and-comparing-algorithms"><a href="#1-Framework-for-predicting-performance-and-comparing-algorithms" class="headerlink" title="1. Framework for predicting performance and comparing algorithms"></a>1. Framework for predicting performance and comparing algorithms</h3><ul><li>Observe feature of natural world</li><li>Hypothesize a model that is consistent with the observations</li><li>Predict events using the hypothesis</li><li>Verify the predictions</li><li>validate by repeating</li></ul><h3 id="2-Calculate-Running-Time"><a href="#2-Calculate-Running-Time" class="headerlink" title="2. Calculate Running Time"></a>2. Calculate Running Time</h3><ol><li><p>Cost Model: Use some basic operation as a proxy for running time, for exampel, the array access <code>if (a[i] + a[j] == 0)</code> under for loop.</p></li><li><p>Tilde Notation: Estimate running time (or memory) as a function of input size N. Then ignore lower order terms</p></li></ol><h2 id="Order-of-Growth"><a href="#Order-of-Growth" class="headerlink" title="Order of Growth"></a>Order of Growth</h2><ol><li>Common classifications<br>$1, log N, N, NlogN, N^2, N^3, and 2^N$</li></ol><p>e.g. Binary search:<br>1) P can be in N distinct indexes from 0 to N-1<br>2) P is not present in the list</p><hr><h1 id="Section-II-Fundamentals"><a href="#Section-II-Fundamentals" class="headerlink" title="Section II: Fundamentals"></a>Section II: Fundamentals</h1><h2 id="Steps-for-buiding-algorithm"><a href="#Steps-for-buiding-algorithm" class="headerlink" title="Steps for buiding algorithm:"></a>Steps for buiding algorithm:</h2><ul><li>Model the problem</li><li>Find an algorithm</li><li>Fast? Fits in memory?</li><li>Find a why to address problem if not, iterate untill satisified</li></ul><hr><h2 id="1-Union-Find"><a href="#1-Union-Find" class="headerlink" title="1. Union-Find"></a>1. Union-Find</h2><h3 id="1-1-Dynamic-connectivity"><a href="#1-1-Dynamic-connectivity" class="headerlink" title="1.1 Dynamic connectivity"></a>1.1 Dynamic connectivity</h3><h4 id="Problem-desceription"><a href="#Problem-desceription" class="headerlink" title="Problem desceription:"></a><em>Problem desceription</em>:</h4><p>Given a set of N objects.</p><ul><li>Union command: connect two objects.</li><li>Find/connected query: is there a path connecting the two objects?</li></ul><h4 id="Problem-Modeling"><a href="#Problem-Modeling" class="headerlink" title="Problem Modeling:"></a><em>Problem Modeling</em>:</h4><p><em>“is connected to”</em>:</p><ul><li>Reflexive: p is connected to p.</li><li>Symmetric: if p is connected to q, then q is connected to p.</li><li>Transitive: if p is connected to q and q is connected to r,<br>then p is connected to r</li></ul><p><em>Connected components</em>: Maximal set of objects that are mutually<br>connected.</p><p><img src="/2023/06/26/Algorithms/1.jpg" alt="Fig.1"></p><p><em>Find query</em>: Check if two objects are in the same component.</p><p><em>Union</em>: Replace compenents containing two obejects with their union. i.e., if we union(2,5) for Figure 1</p><p><em>Union-find data type</em>:<br><img src="/2023/06/26/Algorithms/2.jpg" alt="Fig.2"></p><h3 id="1-2-Solution-1-Quick-find-eager-approach"><a href="#1-2-Solution-1-Quick-find-eager-approach" class="headerlink" title="1.2 Solution 1: Quick find (eager approach)"></a>1.2 Solution 1: Quick find (eager approach)</h3><p><em>Data Structure</em>:</p><ul><li>Integer array id[] of size N.</li><li>Interpretation: p and q are cooneted iff have the same id.<br><img src="/2023/06/26/Algorithms/3.jpg" alt="Fig.3"></li></ul><p><em>Solution</em>:</p><ul><li>Find: Check if p and q have the same id</li><li>Union: If union p and q, change id[p] to id[q]</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">QuickFindUF</span>&#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="type">int</span>[] id;</span><br><span class="line"></span><br><span class="line">    pulic <span class="title function_">QuickFindUF</span><span class="params">(<span class="type">int</span> N)</span></span><br><span class="line">    &#123;</span><br><span class="line">            id = <span class="keyword">new</span> <span class="title class_">int</span>[N];</span><br><span class="line">            <span class="keyword">for</span> (<span class="type">int</span> <span class="variable">i</span> <span class="operator">=</span> <span class="number">0</span>; i&lt;N; i++&gt;) id[i] = i;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="type">boolean</span> <span class="title function_">connected</span><span class="params">(<span class="type">int</span> p, <span class="type">int</span> q)</span>&#123;</span><br><span class="line">        <span class="keyword">return</span> id[p] == id[q]</span><br><span class="line">    &#125;    </span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">union</span><span class="params">(<span class="type">int</span> p, <span class="type">int</span> q)</span>&#123;</span><br><span class="line">        <span class="type">int</span> <span class="variable">pid</span> <span class="operator">=</span> id[p];</span><br><span class="line">        <span class="type">int</span> <span class="variable">qid</span> <span class="operator">=</span> idp[q];</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> = <span class="number">0</span>; i &lt; id.leangth; i++ )&#123;</span><br><span class="line">            <span class="keyword">if</span> (id[i]==pid) id[i] = qid;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="1-3-Solution-2-Quick-union"><a href="#1-3-Solution-2-Quick-union" class="headerlink" title="1.3 Solution 2: Quick union"></a>1.3 Solution 2: Quick union</h3><p><em>Data Structure</em>: </p><ul><li><font color="blue">Integer</font> array id[] of size N</li><li>id[i] is parent of i (continue untill id[i]==i), so the root of i is id[id[…id[i]…]]</li></ul><p><em>Solution</em>:</p><ul><li>Find: Check if p and q have the same root</li><li>Union: Set the if of p’s root to q’s root  </li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">QuickFindUF</span>&#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="type">int</span>[] id;</span><br><span class="line"></span><br><span class="line">    pulic <span class="title function_">QuickFindUF</span><span class="params">(<span class="type">int</span> N)</span></span><br><span class="line">    &#123;</span><br><span class="line">        id = <span class="keyword">new</span> <span class="title class_">int</span>[N];</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> <span class="variable">i</span> <span class="operator">=</span> <span class="number">0</span>; i&lt;N; i++&gt;) id[i] = i;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="type">int</span> <span class="title function_">root</span><span class="params">(<span class="type">int</span> i)</span>&#123;</span><br><span class="line">        <span class="keyword">while</span> (i != id[i]) i = id[i];</span><br><span class="line">        <span class="keyword">return</span> i;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="type">boolean</span> <span class="title function_">connected</span><span class="params">(<span class="type">int</span> p, <span class="type">int</span> q)</span>&#123;</span><br><span class="line">        <span class="keyword">return</span> root(p) == root(q);</span><br><span class="line">    &#125;    </span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">union</span><span class="params">(<span class="type">int</span> p, <span class="type">int</span> q)</span>&#123;</span><br><span class="line">        <span class="type">int</span> <span class="variable">i</span> <span class="operator">=</span> root(p);</span><br><span class="line">        <span class="type">int</span> <span class="variable">j</span> <span class="operator">=</span> root(q);</span><br><span class="line">        id[i] = j</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="1-4-Solution-3-Quick-union-improved"><a href="#1-4-Solution-3-Quick-union-improved" class="headerlink" title="1.4 Solution 3: Quick union improved"></a>1.4 Solution 3: Quick union improved</h3><p><em>Imporve 1: weighting</em>:</p><ul><li>Keep track of <strong>size</strong> of each tree, balance by linking root of smaller tree to larger tree (determine which tree is bigger)</li></ul><p><em>Solution</em>:</p><ul><li>Data structure: based on quick-union, maintain extra array sz[i] to count number of objects in the tree </li><li>Find: identical to quick-union</li><li>Union: <ul><li>Link root of smaller tree to lager</li><li>Update the sz[] array</li></ul></li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">int</span> <span class="variable">i</span> <span class="operator">=</span> root(p);</span><br><span class="line"><span class="type">int</span> <span class="variable">j</span> <span class="operator">=</span> root(q);</span><br><span class="line"><span class="keyword">if</span> (i == j) <span class="keyword">return</span>;</span><br><span class="line"><span class="keyword">if</span> (sz[i] &lt; sz[j]) &#123;id[i] = j; sz[j] += sz[i] &#125;</span><br><span class="line"><span class="keyword">else</span> &#123;id[j] = i; sz[i] += sz[j] &#125;</span><br></pre></td></tr></table></figure><p><em>Running time</em>:</p><ul><li>Find: takes time proportional to depth of p and q</li><li>Union: takes constant time, given roots</li></ul><p><em>Proposition</em>: Depth of any node x is at most $lgN$(base-2 logarithm). The least number of nodes to get a tree with height h is $2^(h-1)$, so $h = lgN + 1 $\approx$ logN $ (PS: the height won’t increase unless two trees have the same height)<br><img src="/2023/06/26/Algorithms/4.jpg" alt="Fig.4"></p><p><em>Improve 2: path compression</em></p><ul><li>After computing the root of p, set the id of each examined node to point to that root.</li><li>Make every other node in path points to its grandparents</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="type">int</span> <span class="title function_">root</span><span class="params">(<span class="type">int</span> i)</span>&#123;</span><br><span class="line">    <span class="keyword">while</span> (i != id[i]) &#123;</span><br><span class="line">        id[i] = id[id[i]];</span><br><span class="line">        i = id[i];</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> i;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><em>order of growth of number of array accesses</em>:</p><div class="table-container"><table><thead><tr><th>algorithm</th><th>initialize</th><th>union</th><th>find</th><th>worst-case time</th></tr></thead><tbody><tr><td>quick-find</td><td>N</td><td>N</td><td>1</td><td>MN</td></tr><tr><td>quick-union</td><td>N</td><td>N</td><td>N</td><td>MN</td></tr><tr><td>weighted QU</td><td>N</td><td>lgN</td><td>lgN</td><td>N + MlogN</td></tr><tr><td>QU + path compression</td><td>N</td><td>\</td><td>\</td><td>N + MlogN</td></tr><tr><td>weighted q-u</td><td>N</td><td>\</td><td>\</td><td>N + MlogN</td></tr></tbody></table></div><p><em>Disadvantage</em>:</p><ul><li>Quick-find: Union is too expensive, trees are flat (takes $N^2$ array access to process N union commands on N objects)</li><li>Quick Union: Trees can get tall. The find could be too expensive</li></ul><h3 id="1-5-QU-Applications"><a href="#1-5-QU-Applications" class="headerlink" title="1.5 QU Applications"></a>1.5 QU Applications</h3><h4 id="1-5-1-Percolation"><a href="#1-5-1-Percolation" class="headerlink" title="1.5.1 Percolation"></a>1.5.1 Percolation</h4>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;The great algorithms are the poetry of computation - Francis Sullivan&lt;/p&gt;
&lt;h1 id=&quot;Section-I-Algorithms-Analysis&quot;&gt;&lt;a href=&quot;#Section-I-Algo</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>Hello World</title>
    <link href="http://example.com/2023/06/24/hello-world/"/>
    <id>http://example.com/2023/06/24/hello-world/</id>
    <published>2023-06-24T14:04:42.840Z</published>
    <updated>2023-06-24T14:04:42.840Z</updated>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;Welcome to &lt;a href=&quot;https://hexo.io/&quot;&gt;Hexo&lt;/a&gt;! This is your very first post. Check &lt;a href=&quot;https://hexo.io/docs/&quot;&gt;documentation&lt;/a&gt; for</summary>
      
    
    
    
    
  </entry>
  
</feed>
